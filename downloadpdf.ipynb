{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOnEIWWNrlYfwZwXSKBhuF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SrijitaThakur/downloadpdf/blob/main/downloadpdf.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gYuifwF6YDMu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory for saving text files\n",
        "text_dir = '/content/sample_data/text'\n",
        "os.makedirs(text_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "lZTEk7xG4dOu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/sample_data/article_for_pets.csv')"
      ],
      "metadata": {
        "id": "05HyGBBN4gLd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to extract the last word from the URL\n",
        "def extract_leaf_node(url):\n",
        "\n",
        "  \"\"\"Extracts the leaf node of a URL.\n",
        "\n",
        "  A leaf node is the last segment of a URL, after the final slash.\n",
        "\n",
        "  Args:\n",
        "    url: The URL to extract the leaf node from.\n",
        "\n",
        "  Returns:\n",
        "    The leaf node of the URL.\n",
        "  \"\"\"\n",
        "\n",
        "  # Get the last segment of the URL.\n",
        "\n",
        "  leaf_node = url.split('/')[-1]\n",
        "\n",
        "  # Return the leaf node.\n",
        "\n",
        "  return leaf_node"
      ],
      "metadata": {
        "id": "lQBV9lAt4vc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to download and save articles as plain text\n",
        "def download_and_save_text(url, text_dir):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        content = response.text\n",
        "\n",
        "        # Parse the HTML content using BeautifulSoup\n",
        "        soup = BeautifulSoup(content, 'html.parser')\n",
        "\n",
        "        # Extract text from specific tags like <p> and <h1> and join with newlines\n",
        "        selected_tags = ['p', 'h1']\n",
        "        text_content = '\\n'.join(tag.get_text() for tag in soup.find_all(selected_tags))\n",
        "        # Remove special characters and retain alphanumeric and common punctuation\n",
        "        text_content = re.sub(r'[^a-zA-Z0-9\\s.,!?;:()\"\\']', '', text_content)\n",
        "\n",
        "        # Extract the last word from the URL to use as the filename\n",
        "        filename = extract_leaf_node(url)\n",
        "        text_filename = os.path.join(text_dir, f'{filename}.txt')\n",
        "\n",
        "        # Save the plain text content to a file\n",
        "        with open(text_filename, 'w', encoding='utf-8') as text_file:\n",
        "            text_file.write(text_content)\n",
        "\n",
        "        return text_filename\n",
        "    return None"
      ],
      "metadata": {
        "id": "YmSa_8eM4zup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Loop through the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    url = row['Article URL']\n",
        "    print(url)\n",
        "\n",
        "    text_filename = download_and_save_text(url, text_dir)\n",
        "\n",
        "    if text_filename:\n",
        "        # Update the DataFrame with the local file name\n",
        "        df.at[index, 'local_file'] = text_filename\n"
      ],
      "metadata": {
        "id": "nycfX_zl43WC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the updated DataFrame to a CSV file\n",
        "df.to_csv('/content/sample_data/updated_data.csv', index=False)"
      ],
      "metadata": {
        "id": "eyOsDt-O3GrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Description:**\n",
        "\n",
        "This Python script is designed to extract and save the content of web pages as plain text files while removing unwanted elements and special characters.\n",
        "\n",
        "---\n",
        "\n",
        "The script performs the following steps:\n",
        "\n",
        "**Setup:**\n",
        "\n",
        "Create a directory for saving text files, specified as text_dir, and ensure that it exists.\n",
        "Read a CSV file containing a list of URLs (e.g., 'article_for_pets.csv') and store the data in a Pandas DataFrame.\n",
        "\n",
        "**Data Processing:**\n",
        "\n",
        "Define a function to extract the last word from a URL, which will be used as the filename.\n",
        "Define a function to download and save web pages as plain text.\n",
        "Download the web page content using the requests library and parse it with BeautifulSoup.\n",
        "\n",
        "**Content Extraction:**\n",
        "\n",
        "Extract text content from specific HTML tags such as paragraphs and h1 and join them using newline characters.\n",
        "\n",
        "Remove special characters and retain alphanumeric characters, common punctuation, spaces, and newline characters using regular expressions.\n",
        "\n",
        "**File Saving:**\n",
        "\n",
        "Save the plain text content to a text file with a filename derived from the last word in the URL.\n",
        "\n",
        "**Processing Loop:**\n",
        "\n",
        "Loop through the DataFrame, processing each URL one by one.\n",
        "Print the URL being processed.\n",
        "DataFrame Update:\n",
        "\n",
        "Update the DataFrame with the local file name of the saved text file for each URL.\n",
        "Data Export:\n",
        "\n",
        "Save the updated DataFrame to a CSV file for future reference.\n",
        "The resulting plain text files are saved in the specified text_dir directory. This script enables you to convert web page content into clean and structured plain text format for further analysis or archiving, while excluding navigation elements, images, ads, and special characters.\n"
      ],
      "metadata": {
        "id": "Cw65hKQb4_uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r sample_data/requirements.txt"
      ],
      "metadata": {
        "id": "2Dn7HqELpF_r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}